{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train a GPT-2 Text-Generating Model w/ GPU","provenance":[{"file_id":"1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce","timestamp":1581973519656},{"file_id":"1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK","timestamp":1555602712120}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"H7LoMj4GA4n_","colab_type":"text"},"source":["#  Train a GPT-2 Text-Generating Model w/ GPU\n","\n","Taken from work by [Max Woolf](http://minimaxir.com)\n","\n","\n","Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n","\n","For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read my [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n","\n","\n","To get started:\n","\n","1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n","2. Make sure you're running the notebook in Google Chrome.\n","3. Run the cells below:\n"]},{"cell_type":"code","metadata":{"id":"KBkpRgBCBS2_","colab_type":"code","outputId":"28f785bb-e099-4e29-8552-fc37161a9d1a","executionInfo":{"status":"ok","timestamp":1587229773630,"user_tz":240,"elapsed":7572,"user":{"displayName":"Lucas Liu","photoUrl":"","userId":"15250186642363235202"}},"colab":{"base_uri":"https://localhost:8080/","height":182}},"source":["%tensorflow_version 1.x\n","!pip install -q gpt-2-simple\n","import gpt_2_simple as gpt2\n","from datetime import datetime\n","from google.colab import files"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bj2IJLHP3KwE","colab_type":"text"},"source":["## GPU\n","\n","Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n","\n","You can verify which GPU is active by running the cell below."]},{"cell_type":"code","metadata":{"id":"sUmTooTW3osf","colab_type":"code","outputId":"8a98e8b7-56c3-4fa5-a09f-d81dd9782695","executionInfo":{"status":"ok","timestamp":1567013419709,"user_tz":420,"elapsed":1351,"user":{"displayName":"Max Woolf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mD2mXear_qVUygpwsjIX8bFdN6WN2fKW_XEDgsFOA=s64","userId":"10954469476206133987"}},"colab":{"base_uri":"https://localhost:8080/","height":311}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Wed Aug 28 17:30:18 2019       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   51C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0wXB05bPDYxS","colab_type":"text"},"source":["## Downloading GPT-2\n","\n","If you're retraining a model on new text, you need to download the GPT-2 model first. \n","\n","There are three released sizes of GPT-2:\n","\n","* `124M` (default): the \"small\" model, 500MB on disk.\n","* `355M`: the \"medium\" model, 1.5GB on disk.\n","* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n","* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n","\n","Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n","\n","The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n","\n","This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."]},{"cell_type":"code","metadata":{"id":"P8wSlgXoDPCR","colab_type":"code","outputId":"08afb151-00ba-4781-cf38-6e98de1e5599","executionInfo":{"status":"ok","timestamp":1587228315862,"user_tz":240,"elapsed":4153,"user":{"displayName":"Lucas Liu","photoUrl":"","userId":"15250186642363235202"}},"colab":{"base_uri":"https://localhost:8080/","height":146}},"source":["gpt2.download_gpt2(model_name=\"124M\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Fetching checkpoint: 1.05Mit [00:00, 213Mit/s]                                                      \n","Fetching encoder.json: 1.05Mit [00:00, 89.2Mit/s]                                                   \n","Fetching hparams.json: 1.05Mit [00:00, 366Mit/s]                                                    \n","Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 187Mit/s]                                   \n","Fetching model.ckpt.index: 1.05Mit [00:00, 184Mit/s]                                                \n","Fetching model.ckpt.meta: 1.05Mit [00:00, 172Mit/s]                                                 \n","Fetching vocab.bpe: 1.05Mit [00:00, 168Mit/s]                                                       \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"N8KXuKWzQSsN","colab_type":"text"},"source":["## Mounting Google Drive\n","\n","The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n","\n","Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"]},{"cell_type":"code","metadata":{"id":"puq4iC6vUAHc","colab_type":"code","outputId":"73e41ae4-9f77-4b29-9397-32c615dc46f9","executionInfo":{"status":"ok","timestamp":1587226518326,"user_tz":240,"elapsed":22055,"user":{"displayName":"Lucas Liu","photoUrl":"","userId":"15250186642363235202"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["gpt2.mount_gdrive()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BT__brhBCvJu","colab_type":"text"},"source":["## Uploading a Text File to be Trained to Colaboratory\n","\n","In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n","\n","![alt text](https://i.imgur.com/TGcZT4h.png)\n","\n","Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."]},{"cell_type":"code","metadata":{"id":"6OFnPCLADfll","colab_type":"code","colab":{}},"source":["file_name = \"shakespeare.txt\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HeeSKtNWUedE","colab_type":"text"},"source":["If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."]},{"cell_type":"code","metadata":{"id":"-Z6okFD8VKtS","colab_type":"code","colab":{}},"source":["gpt2.copy_file_from_gdrive(file_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LdpZQXknFNY3","colab_type":"text"},"source":["## Finetune GPT-2\n","\n","The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n","\n","The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n","\n","The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n","\n","**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n","\n","Other optional-but-helpful parameters for `gpt2.finetune`:\n","\n","\n","*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n","* **`sample_every`**: Number of steps to print example output\n","* **`print_every`**: Number of steps to print training progress.\n","* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n","*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n","* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "]},{"cell_type":"code","metadata":{"id":"aeXshJM-Cuaf","colab_type":"code","outputId":"d7276e8d-2067-415f-8d55-d2551391d4f7","executionInfo":{"status":"ok","timestamp":1587229698384,"user_tz":240,"elapsed":1371181,"user":{"displayName":"Lucas Liu","photoUrl":"","userId":"15250186642363235202"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["sess = gpt2.start_tf_sess()\n","\n","gpt2.finetune(sess,\n","              dataset=file_name,\n","              model_name='124M',\n","              steps=1000,\n","              restore_from='latest',\n","              run_name='run1',\n","              print_every=10,\n","              sample_every=200,\n","              save_every=100,\n","              overwrite=True\n","              )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Loading checkpoint models/124M/model.ckpt\n","INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loading dataset...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["dataset has 338025 tokens\n","Training...\n","Saving checkpoint/run1/model-0\n","[10 | 18.93] loss=3.57 avg=3.57\n","[20 | 31.55] loss=3.28 avg=3.43\n","[30 | 44.15] loss=3.41 avg=3.42\n","[40 | 56.78] loss=3.33 avg=3.40\n","[50 | 69.36] loss=3.20 avg=3.36\n","[60 | 81.99] loss=3.40 avg=3.36\n","[70 | 94.57] loss=3.25 avg=3.35\n","[80 | 107.14] loss=3.26 avg=3.34\n","[90 | 119.76] loss=3.31 avg=3.33\n","[100 | 132.44] loss=3.20 avg=3.32\n","Saving checkpoint/run1/model-100\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","[110 | 148.11] loss=3.06 avg=3.29\n","[120 | 160.74] loss=3.04 avg=3.27\n","[130 | 173.40] loss=3.20 avg=3.27\n","[140 | 186.05] loss=3.00 avg=3.25\n","[150 | 198.66] loss=3.27 avg=3.25\n","[160 | 211.32] loss=2.97 avg=3.23\n","[170 | 223.98] loss=2.99 avg=3.21\n","[180 | 236.66] loss=2.99 avg=3.20\n","[190 | 249.24] loss=2.86 avg=3.18\n","[200 | 261.81] loss=3.23 avg=3.18\n","Saving checkpoint/run1/model-200\n","======== SAMPLE 1 ========\n","reges I'll do\n","The thing which my master had already set me off with;\n","And then my master's tongue would take it all.\n","\n","BRUTUS:\n","Do it not with words, Brutus! speak it with words.\n","\n","First Citizen:\n","Ay, that they are,\n","And for my master's sake.\n","\n","Third Citizen:\n","You speak of the Roman state more than I mean.\n","\n","GLUCENIA:\n","I say, you take your tongue to your word.\n","\n","First Citizen:\n","O,\n","What tongue you speak of?\n","\n","Second Citizen:\n","O,\n","How now, you two,\n","That tongue you speak so like\n","Makes you think but little; you speak of the\n","Roman state; indeed, you spoke that of\n","that state of yours; for you, I pray,\n","Were like your master the more in this;\n","That, if you spoke of your mother, you must\n","have given up all the use of them; for they are\n","much more than you were, nor less than you.\n","\n","GLUCENIA:\n","Nay, go see!\n","\n","Third Citizen:\n","For all the more reason; to have a state where you know\n","\n","GLUCENIA:\n","But you are bound to the old way.\n","\n","First Citizen:\n","Nay, I know not your reason.\n","\n","Second Citizen:\n","Why, your reason.\n","\n","First Citizen:\n","Your opinion, sir, is as it is for\n","any thing that shall be done in this province.\n","\n","Second Citizen:\n","Your argument?\n","\n","First Citizen:\n","You will not know how much it shall be.\n","\n","Second Citizen:\n","You may, sir; but I'll give it you a beat.\n","\n","First Citizen:\n","But, sir, you may.\n","\n","Second Citizen:\n","You may not, sir; but I'll give it one\n","pinch.\n","\n","First Citizen:\n","O, then you have got your advantage. You\n","cannot have your own reason why you can't, and\n","you are not the only.\n","\n","Second Citizen:\n","You can, sir.\n","\n","First Citizen:\n","But you are the only.\n","\n","Second Citizen:\n","You are the only.\n","\n","First Citizen:\n","\n","Third Citizen:\n","It is the only.\n","\n","First Citizen:\n","That we are; and the other.\n","\n","Second Citizen:\n","They are.\n","\n","First Citizen:\n","That you are.\n","\n","Third Citizen:\n","No, sir.\n","\n","First Citizen:\n","You shall know it.\n","\n","Third Citizen:\n","No, sir.\n","\n","First Citizen:\n","No, sir; you shall not, sir; my reason is; sir.\n","\n","Second Citizen:\n","They are all.\n","\n","Third Citizen:\n","And my reason is their own.\n","\n","First Citizen:\n","The worse for your comfort.\n","\n","First Citizen:\n","You shall.\n","\n","Third Citizen:\n","The better, sir.\n","\n","First Citizen:\n","The better for your comfort.\n","All hail, sir, and thanks!\n","\n","Third Citizen:\n","Thanks.\n","\n","All hail, sir; good night\n","and merry good morrow!\n","\n","First Citizen:\n","Good night all!\n","The night is short in this country; all that is\n","present, being in the past, is past, present, past.\n","\n","Second Citizen:\n","The past will be past.\n","\n","First Citizen:\n","The past is past. Then let's not go hence.\n","\n","Third Citizen:\n","Away! it is past.\n","\n","First Citizen:\n","Ay, my good night, and a happy night!\n","\n","Third Citizen:\n","So will it.\n","\n","First Citizen:\n","So shall it.\n","\n","Third Citizen:\n","Now, I know not what.\n","\n","First Citizen:\n","Then I shall not answer, nor do me any\n","thing.\n","\n","Third Citizen:\n","Then I know not what: but if I do, I know it\n","will be like so: it will be like such a thing:\n","but it is much more; and therefore much better.\n","\n","First Citizen:\n","More than more; but the more you know it,\n","the better you will; for a goodly, and not a\n","frightening, is not so much to be say, 'Good evening,'\n","'Good night,' not so much. Is nothing now a goodly?\n","'Never, sir.'\n","Second Citizen:\n","'Never, sir.'\n","\n","First Citizen:\n","'No, sir'--the good sense to be honest\n","is well known: and being honest is such a thing\n","that, being goodly, it doth deserve a goodly;\n","and being otherwise, being otherwise, it doth not deserve a\n","goodly.\n","\n","First Citizen:\n","\n","\n","[210 | 289.67] loss=2.65 avg=3.16\n","[220 | 302.25] loss=2.45 avg=3.12\n","[230 | 314.85] loss=3.14 avg=3.12\n","[240 | 327.54] loss=2.66 avg=3.10\n","[250 | 340.15] loss=2.71 avg=3.08\n","[260 | 352.73] loss=2.80 avg=3.07\n","[270 | 365.34] loss=2.66 avg=3.05\n","[280 | 377.95] loss=2.82 avg=3.04\n","[290 | 390.59] loss=2.68 avg=3.03\n","[300 | 403.16] loss=2.79 avg=3.02\n","Saving checkpoint/run1/model-300\n","[310 | 418.29] loss=2.49 avg=3.00\n","[320 | 430.87] loss=2.36 avg=2.98\n","[330 | 443.46] loss=2.62 avg=2.96\n","[340 | 456.12] loss=2.22 avg=2.94\n","[350 | 468.73] loss=2.32 avg=2.92\n","[360 | 481.33] loss=2.38 avg=2.90\n","[370 | 493.99] loss=2.38 avg=2.88\n","[380 | 506.65] loss=2.26 avg=2.86\n","[390 | 519.32] loss=2.34 avg=2.85\n","[400 | 531.99] loss=2.23 avg=2.83\n","Saving checkpoint/run1/model-400\n","======== SAMPLE 1 ========\n"," towards\n","The holy vow I made. What did thou,\n","The mortal and the mortal's tongue do here,\n","To have their holy writ? What did thou, O that\n","This penitent life can do here? The oath\n","Which was made, I think, with the best discretion,\n","And then 'twill obey? There's no other word for it\n","But 'solemnity.' Thou wast the mortal:\n","Thou drew forth, thou hast seen the work, thou\n","Destroy'd the mortal. Thou hast the holy life, thou\n","The mortal, the mortal's life; and thou, O the\n","flesh that didst afflict thee so I deserve,\n","In this life, to come hence to life again,\n","Not being of the mortal's body, thou that didst afflict\n","The flesh. But thou didst seal this off, thou\n","Vouchsafe to come hither to do that damn'd deed\n","Whose name I'll not name.\n","\n","VOLUMNIA:\n","O Jupiter's daughter! tell me of this.\n","\n","VOLUMNIA:\n","My mother's lie!\n","Away! Let her speak.\n","\n","VOLUMNIA:\n","Hark her! I will follow her.\n","\n","DION:\n","My lord!\n","\n","DION:\n","\n","VOLUMNIA:\n","Thou canst not only tell of her falsehood, she tells it\n","Withal. But thou canst not only make thee a lie,\n","And deceive thy people, thou but make me a liar\n","And sell my falsehood as counterfeit: I am a Roman\n","I have known,\n","And one that is a friend to Rome; and if I should\n","Have been a Christian on the Volscian side,\n","I had deserved a fine and a year in prison\n","Till this did pass away. This is Carthage, I\n","I am sure of it, and by the report of the\n","victim, Phaethon. Carthage, where I am\n","I will find it, with Carthage, lies Phaethon;\n","If I keep my oath, I will stay my stay there\n","And make you a Roman on either side.\n","\n","MENENIUS:\n","If you can keep your oath,\n","You shall not only triumph on either side,\n","But you shall reap the privilege that you had\n","Even when you were the conqueror: so you are,\n","If you will not part with Carthage,\n","There shall be liberty for you hereafter. If\n","you please.\n","\n","MENENIUS:\n","Go, to the temple; I will follow after thee\n","As I follow after my master's wife: I'll here stand\n","Upon one gate that seals my entrance.\n","There must I look, to take thee to thy grave: here thou take,\n","And this grave thou lead, that thou mayst not look upon,\n","But, if thou be but the vice, and yet thy soul art\n","So strong as lightning, take thee hence.\n","\n","VOLUMNIA:\n","My lord! I will;\n","For I must follow, and if I be deceived,\n","I must not stay here: but if thou tell'st\n","me by whom, my oath, I should stay, thou know'st\n","me a falsehood enough; and if I would wrong,\n","Thou shouldst break one of the cardinal parts, which\n","I swore I should keep. I will be your bawd, so\n","be it as it appears.\n","\n","Shepherd:\n","My lord, we will.\n","\n","VOLUMNIA:\n","As much as you will follow us; so you must follow us:\n","If you do not then, we will be thy gate, and with\n","your patience we'll give in. What way were I to enter?\n","\n","POLIXENES:\n","To the temple\n","Or by sea to another city\n","Or by conveyance to Phaethon.\n","\n","CHAP. IX:\n","How now, my lord! what say you to my father?\n","\n","CORIOLANUS:\n","Pray thou, I pray, please to know his mind.\n","\n","VOLUMNIS:\n","He is the state's soldier.\n","Here in Rome he would seem too dangerous.\n","\n","CORIOLANUS:\n","We have it; but I cannot determine\n","His nature.\n","\n","VOLUMNIS:\n","He makes a prey of my request.\n","\n","CORIOLANUS:\n","He will not fail the fray. But, my lord,\n","He that comes first must suffer no exceptions.\n","\n","VOLUMNIS:\n","He will go: the common tribunes have ta'en an hour,\n","The parliament a day, his fellow-country a month,\n","The senate a year; who having trow'd not time to do\n","Their errand, come upon him. What say you\n","to\n","\n","[410 | 558.96] loss=2.13 avg=2.81\n","[420 | 571.58] loss=2.26 avg=2.79\n","[430 | 584.21] loss=2.28 avg=2.78\n","[440 | 596.87] loss=2.37 avg=2.77\n","[450 | 609.57] loss=2.19 avg=2.75\n","[460 | 622.24] loss=2.04 avg=2.73\n","[470 | 634.91] loss=2.07 avg=2.71\n","[480 | 647.66] loss=1.72 avg=2.69\n","[490 | 660.34] loss=1.50 avg=2.66\n","[500 | 673.03] loss=2.03 avg=2.64\n","Saving checkpoint/run1/model-500\n","[510 | 688.36] loss=1.72 avg=2.62\n","[520 | 701.05] loss=1.73 avg=2.60\n","[530 | 713.70] loss=1.55 avg=2.57\n","[540 | 726.32] loss=1.71 avg=2.55\n","[550 | 738.91] loss=1.74 avg=2.53\n","[560 | 751.53] loss=1.30 avg=2.50\n","[570 | 764.18] loss=1.73 avg=2.49\n","[580 | 776.75] loss=1.32 avg=2.46\n","[590 | 789.34] loss=1.33 avg=2.43\n","[600 | 801.95] loss=1.25 avg=2.41\n","Saving checkpoint/run1/model-600\n","======== SAMPLE 1 ========\n","our't have seen the sun that shines so: nor have we,\n","Nor have we read of the coming wars, nor read of\n","the coming wars, not been in sight of the sun\n","that shines so: these are merely my sharrings:\n","I dare my life maintain them.\n","\n","BENVOLIO:\n","Go, fight it, Benvolio.\n","\n","ROMEO:\n","I would my fortune have worn out the shame of delay,\n","Which I still have withsworn to give you. You are like\n","I went from the Sicils to the Cupid's home,\n","And received a poor parcel of rotten flesh:\n","An hour after I left, the day going by,\n","What I now crave was my fortune, which being graced\n","With such delicacy as is wont to do,\n","To dispatch me to the Caperio; there I\n","Bid the lady whom you accuse deliverance,\n","And left me with much o'er her; though she promised\n","To have me at the appointed time.\n","Yet, what though I do not expressly say\n","Let her be deliver'd to the Capulets,\n","And that she is to appear in the Capitol\n","Upon the four winds that blow forth the air?\n","\n","LARTIGUEL:\n","What, mean not me!\n","When then I shall not be deliver'd to the\n","Caperio?\n","\n","ROMEO:\n","I, mean not thee, nor any of my lieges.\n","\n","LARTIGUR:\n","If, in the end, thou art all unjust,\n","Let me have no disseisinues against thee;\n","Pursue, if you may, the other ends that follow.\n","\n","First Servant:\n","I thank you, good cousin Benvolio.\n","\n","Second Servant:\n","Good aim, cousin Benvolio.\n","\n","POMPEY:\n","What, pray, fie! so, so!\n","\n","CAPULET:\n","Not all, I warrant, are equal.\n","\n","LARTIGUEL:\n","You have not duly received the letters of torture\n","Which, you know, the friends of the state\n","Have begun to your attorneys' attempts.\n","\n","CAPULET:\n","How now, Montague!\n","What, wherefore? is torture heinous according to the law?\n","Why, then, is it against the law?\n","Then, for this, thou shalt be a mourner\n","And a traitor, if thou yield'st this place\n","To the Capulets: thou makest no attempt\n","To break the law, if thou darest.\n","Your pardon, Camillo!\n","\n","CAPULET:\n","What, pardon, Montague!\n","Thou art the cause o' the torture; and, in the end,\n","So cruel and so harsh, no wayman could come\n","But by the other ends.\n","\n","Nurse:\n","What, art thou dead?\n","\n","Nurse:\n","I saw your face on the corse; and I say again:\n","'Alack, for good measure thou art dead; and I say again\n","Thou art dead: thou art so great, I can hardly believe\n","I was ever alive. I can think only\n","The devil disguising thee, now thy master,\n","Hath obscured thy state: thou art the face, I can\n","But 't be seen to be a saint, for thou\n","Twice was the first mista'en'd; thou seest me\n","Like the very shapes of hell. This is not\n","The first time I did the mock-scafing: my soul\n","Was indeed alive when that myself was mock'd,\n","I must confess: I thought it was a tardiness\n","To be so naked, and therefore naked: but, in my\n","confession, I do confess I did mock,\n","Not because I did, but because\n","Thou said I did.\n","\n","Nurse:\n","O, do so, do you!\n","O Brother, pity you not!\n","My wife accuses thee of witchcraft; thou hast\n","A mortal sin: excuse this by guess;\n","There was no mortal sin; now it is\n","A mortal offence, and every mortal\n","Is guilty of it. O, pardon! O brother,\n","Sister, and sister, go with me; we shall\n","Present thee with thy frets, hands, feet, and so\n","As much evidence against thy brotherhood,\n","As against thy brotherhood.\n","\n","ROMEO:\n","Alack, anon!\n","\n","Nurse:\n","What, art thou dead!\n","\n","ROMEO:\n","Why, sir, I, sir,\n","Were much better, if it had been for a brother,\n","To have caught thy sister's husbandry,\n","To have have strew'd thy compass, and so\n","Have left thy point of aim at thy\n","treasured index finger\n","\n","[610 | 828.55] loss=1.22 avg=2.38\n","[620 | 841.14] loss=1.73 avg=2.37\n","[630 | 853.79] loss=1.56 avg=2.35\n","[640 | 866.36] loss=1.42 avg=2.33\n","[650 | 878.95] loss=1.41 avg=2.31\n","[660 | 891.54] loss=1.09 avg=2.29\n","[670 | 904.21] loss=0.80 avg=2.26\n","[680 | 916.83] loss=1.07 avg=2.23\n","[690 | 929.54] loss=0.76 avg=2.20\n","[700 | 942.18] loss=1.10 avg=2.18\n","Saving checkpoint/run1/model-700\n","[710 | 957.71] loss=0.96 avg=2.16\n","[720 | 970.36] loss=1.02 avg=2.13\n","[730 | 983.04] loss=0.92 avg=2.11\n","[740 | 995.70] loss=1.00 avg=2.09\n","[750 | 1008.33] loss=0.67 avg=2.06\n","[760 | 1020.96] loss=1.01 avg=2.04\n","[770 | 1033.61] loss=0.85 avg=2.02\n","[780 | 1046.29] loss=0.99 avg=2.00\n","[790 | 1058.94] loss=0.97 avg=1.98\n","[800 | 1071.62] loss=0.65 avg=1.96\n","Saving checkpoint/run1/model-800\n","======== SAMPLE 1 ========\n","\n","Away with her, and bid her marry Prince Florizel\n","And keep her safe from the south: for you,\n","I'll not vex her: she is a queen and\n","prince, and my wife shall have full power. But\n","forthwith for your company,--\n","\n","Second Capulet:\n","Here brings forth the queen's blood.\n","\n","CAPULET:\n","Good queen and wife, bring it forth.\n","\n","VIRGILIA:\n","\n","CAPULET:\n","Let's hence, daughter.\n","\n","VIRGILIA:\n","\n","CAPULET:\n","Let's go, let's go.\n","\n","PARIS:\n","Go, get you gone: the lady's with me; I'll speak\n","all in private: my son will be coron'd\n","this day; and I wish him fair the more for his\n","continency.\n","\n","CAPULET:\n","I thank you, gentle mother.\n","Father Cain shall keep the party, as your\n","son is going about, like his youthful spirit well appertains\n"," to me.\n","\n","CAMILLO:\n","Good mother,\n","Go hence; I would not endure the meeting with\n","your son.\n","\n","CAPULET:\n","Now hast thou found the maid? what find'st stream doth send\n","thither; seek her the bitter end of death.\n","\n","PARIS:\n","Saddle thy horse.\n","Fie, fie, fie! Juliet is found!\n","She is as near as I can wish: what\n","soul and what spirit do we not cohabit? Is't not\n","it true that I and Romeo go about to death?\n","\n","Nurse:\n","It is; it is the truth: and so, father, I\n","will cohabit Romeo with those defects which you\n","have coigned up to my death, like enjoying life.\n","\n","CAPULET:\n","Come, we are to die.\n","\n","Nurse:\n","\n","PARIS:\n","Ah, ah, ah! ah, ah! ah, ah! ah!\n","Ah, ah, ah, ah, ah, ah, ah, ah!\n","Ah, ah, ah, ah!\n","Ah, ah!\n","\n","CAPULET:\n","Let us stand, let us be gone, let us die.\n","Ah, ah, ah! Ah, ah!\n","\n","JULIET:\n","\n","CAPULET:\n","Ah, there thou art, there thou comest me:\n","death, for that is nothing but death to thee.\n","\n","JULIET:\n","Ah, there thou art, there thou comest me.\n","\n","CAPULET:\n","Death is nothing but an inhabitant of thee,\n","Even of that same sweet clang which we call home;\n","And thou, a puppet made to die by thine own\n","will, a thing made to tremble at thy intrusion:\n","O, hark! thy breathing grows more and more shallow;\n","The last corns and my old shrift grows less and\n","lesser. My last gasp is like noise made from thee;\n","And then thine oars burst, and the dull woofing\n","of rose-petals. Cuffless Romeo, come forth;\n","Bear with me such companions thou gerests:\n","Be not abhorr'd, but prefer'd and learn'd\n","Like craftily. O, know you how to welcome a\n","creampor? Know you how to charm a tomboy?\n","Henceforth shall you be a charm instead,\n","Nor a playboy, but be a charm o' the\n","mantle, till all the other delights return?\n","\n","Messenger:\n","Welcome, ah, buzzard.\n","\n","CAPULET:\n","O, were there not more reasons why heaven\n","May never make good, it should being so\n","Many complexions yield; and so we be damned,\n","For this most complexion brings us but one joy:\n","To put other cares aside, such joy as this,\n","Must leave heaven's side, and must again use heaven\n","For cares. O traveller!\n","Go not after him, but make way for yourself;\n","You are welcome.\n","\n","Servant:\n","What haste?\n","\n","CAPULET:\n","No longer welcome at all; for then 'tis no\n","fare: you shall have more time to get thee with\n","after us.\n","\n","LEONTES:\n","O, time for some instruction!\n","Thele comest unto your aid, and, good my lord,\n","All my affairs undergo change.\n","Take mine sword, for it competes with my\n","brother's: if you be sudden in bearing him off,\n","It shall serve you well. If you be new to us,\n","it is found that our affairs we follow have been\n","especially\n","to him, that makes us think of him.\n","\n","Messenger\n","\n","[810 | 1098.52] loss=0.81 avg=1.94\n","[820 | 1111.26] loss=0.50 avg=1.91\n","[830 | 1123.88] loss=0.76 avg=1.89\n","[840 | 1136.49] loss=0.61 avg=1.87\n","[850 | 1149.07] loss=0.79 avg=1.85\n","[860 | 1161.68] loss=0.37 avg=1.83\n","[870 | 1174.22] loss=0.44 avg=1.80\n","[880 | 1186.80] loss=0.47 avg=1.78\n","[890 | 1199.41] loss=0.51 avg=1.76\n","[900 | 1211.98] loss=0.50 avg=1.74\n","Saving checkpoint/run1/model-900\n","[910 | 1227.40] loss=0.55 avg=1.72\n","[920 | 1239.99] loss=0.48 avg=1.70\n","[930 | 1252.57] loss=0.56 avg=1.68\n","[940 | 1265.15] loss=0.38 avg=1.66\n","[950 | 1277.75] loss=0.33 avg=1.63\n","[960 | 1290.32] loss=0.42 avg=1.62\n","[970 | 1302.90] loss=0.32 avg=1.59\n","[980 | 1315.50] loss=0.56 avg=1.58\n","[990 | 1328.06] loss=0.32 avg=1.56\n","[1000 | 1340.67] loss=0.40 avg=1.54\n","Saving checkpoint/run1/model-1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IXSuTNERaw6K","colab_type":"text"},"source":["After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n","\n","If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."]},{"cell_type":"code","metadata":{"id":"VHdTL8NDbAh3","colab_type":"code","colab":{}},"source":["gpt2.copy_checkpoint_to_gdrive(run_name='run1')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQJgV_b4bmzd","colab_type":"text"},"source":["You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."]},{"cell_type":"markdown","metadata":{"id":"pel-uBULXO2L","colab_type":"text"},"source":["## Load a Trained Model Checkpoint\n","\n","Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."]},{"cell_type":"code","metadata":{"id":"DCcx5u7sbPTD","colab_type":"code","colab":{}},"source":["gpt2.copy_checkpoint_from_gdrive(run_name='run1')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RTa6zf3e_9gV","colab_type":"text"},"source":["The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n","\n","**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."]},{"cell_type":"code","metadata":{"id":"-fxL77nvAMAX","colab_type":"code","outputId":"4255ef20-3f87-4066-84f8-43e9046842ab","executionInfo":{"status":"ok","timestamp":1587229790351,"user_tz":240,"elapsed":8169,"user":{"displayName":"Lucas Liu","photoUrl":"","userId":"15250186642363235202"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["sess = gpt2.start_tf_sess()\n","gpt2.load_gpt2(sess, run_name='run1')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading checkpoint checkpoint/run1/model-1000\n","INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ClJwpF_ACONp","colab_type":"text"},"source":["## Generate Text From The Trained Model\n","\n","After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."]},{"cell_type":"code","metadata":{"id":"4RNY6RBI9LmL","colab_type":"code","outputId":"f23c7545-cf7b-4dd1-e998-f8ef3db625f1","executionInfo":{"status":"ok","timestamp":1582471778711,"user_tz":300,"elapsed":12176,"user":{"displayName":"Lucas Liu","photoUrl":"","userId":"15250186642363235202"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["gpt2.generate(sess, run_name='news1')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[ female announcer ] the time had come for all america to pay attention to elections day. a new twist on the mid-term election in washington, d.c.'s east, with the election just weeks away. but one person seems determined to make sure that that happens.   and for what?   if we want to go back to our roots as a nation, to what we have always done, which is lead by example, to lead by example, have faith-based communities, that our institutions of higher learning, our trust that our government is there for us, then we have to follow the rules. if we don't, then something is very, very wrong.  i don't know what it is. i don't even know what it is. i love the fact that bill and i haven't had a conversation with anybody about it. it is a very basic principle. but what i think, certainly what everybody has an outrage over, and a deep concern about is the idea that they are being watched. the idea that they are being watched every step of the way, because they are acting out a culture of fear and loathing of different groups. that they are being watched with all the restraint that you would expect from a government agency.  i would like to hear from you on that. thank you so much.  thank you.   a fox news alert. a bombshell report about the internal government of former congressman jesse jackson. what do you make of this? we have the inside story right now.   a classified report that the cia released on jesse jackson. it is shocking. it is shocking because it is socoz there is a level of intimidation and you have got people that are saying clearly, clearly, we are not going to allow this to happen. this is absolutely troubling to anybody. this is a very disturbing situation. this is something that should never have happened. this is something that should never have to happen. this is a very troubling situation and should never have occurred. this is something that should never have to happen in a free society.  jackson's former aide, hannity, joins us. hi.  good to see you.  thanks very much.  now, we're getting a lot of questions about what exactly happened that night. the cia released this statement. it basically read, well, congressman jackson was sort of throwing a fit over inappropriate touching between agyn and another celebrity. the senator took the opportunity to respond, saying, you know what, i'm a littleboy, i don't quite understand what you're saying. i certainly don't quite understand why you're asking. it was not an accident. why would you throw a few more questions?  well, i think what you're really asking is why did this happen? i don't quite understand, though, why you would throw questions about that sort of thing. i do, however, think that you're forgetting a point. this is an important story about the relationship between hollywood and stanford. if you were a studio head in one studio and the network was in the other, you're not going to throw any questions because the people you're trying to get are going to be looking at you curiously. you know what? you're not going to -- you're not going to throw a question. you're not going to throw a question if the person answering the n word is network af. you know what? you're not doing anything wrong.  right, and i think you're forgetting something.  i would.  but, you know, i think you're forgetting something. i think you're forgetting something. and let's face it, if you were a network executive, if you were watching this closely, you would not be throwing questions about that sort of thing. you'd be looking at documents and, for that matter, you would not be looking at programs and programs would be kind of filtering through and finding loopholes and finding loopholes and finding loopholes. and you have got to account, for example, with a lot of these programs that are out there, finding loopholes, finding them in the very programs that you're trying to protect. for example, the fact that you're not going to be diving into legal waters, you know what you're talking about, diving into a constitutional hole. you know what you're talking about. i think that the question is, is there a rationale why you would throw a few more questions about that kind of thing? i mean, is there any reason, you know, i can't just throw a few more questions about this. i mean, is there any rational beyond what you're asking? i mean, you know, excuse me, you, you would be rational people, you know what you're talking about, right? i think you're seeing this, you know, as a pattern emerging, well, you know, and it's not just a question of, you know, i'm you,\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oF4-PqF0Fl7R","colab_type":"text"},"source":["If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n","\n","You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n","\n","You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n","\n","Other optional-but-helpful parameters for `gpt2.generate` and friends:\n","\n","*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n","* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n","* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n","* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n","* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n","*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."]},{"cell_type":"code","metadata":{"id":"8DKMc0fiej4N","colab_type":"code","outputId":"fb5cac42-449b-4f9b-8672-33db5fae8722","executionInfo":{"status":"ok","timestamp":1587230683564,"user_tz":240,"elapsed":13309,"user":{"displayName":"Lucas Liu","photoUrl":"","userId":"15250186642363235202"}},"colab":{"base_uri":"https://localhost:8080/","height":641}},"source":["gpt2.generate(sess,\n","              run_name='run1',\n","              length=250,\n","              temperature=0.8,\n","              prefix=\"JULIET:\\nwherefore art thou Romeo?\\nROMEO:\",\n","              nsamples=1,\n","              batch_size=1\n","              )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["JULIET:\n","wherefore art thou Romeo?\n","ROMEO:\n","think'st thou, Juliet, 'tis not the man's fault,\n","but his wish?\n","\n","POMPEY:\n","I doubt it not, sir. Romeo\n","was never the man's man.\n","\n","ROMEO:\n","Thou didst love him, though he was grown to hate thee:\n","though he was love to himself, yet hast thou made\n","him an enemy to thy devotion:\n","he was a swain, had he not a soul.\n","\n","POMPEY:\n","Mine own too.\n","\n","ROMEO:\n","Well said, prince. My own too! Why go'st of us\n","for help? Where's Romeo? Where is Romeo?\n","What is the king? where's King Aufidius? What's the business\n","constrained? where's Verona? Where's Aufidius?\n","Where are those Capulets, where'e you lions? Where's\n","A Romeo? A Juliet? A Romeo?\n","\n","POMPEY:\n","Where is Aufidius?\n","\n","ROMEO:\n","There is Romeo\n","But not Romeo. Romeo is Aufidius; Romeo is\n","A Juliet; A Romeo;\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zjjEN2Tafhl2","colab_type":"text"},"source":["For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n","\n","You can rerun the cells as many times as you want for even more generated texts!"]},{"cell_type":"code","metadata":{"id":"Fa6p6arifSL0","colab_type":"code","colab":{}},"source":["gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n","\n","gpt2.generate_to_file(sess,\n","                      destination_path=gen_file,\n","                      length=500,\n","                      temperature=0.7,\n","                      nsamples=100,\n","                      batch_size=20\n","                      )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-LRex8lfv1g","colab_type":"code","colab":{}},"source":["# may have to run twice to get file to download\n","files.download(gen_file)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QQAN3M6RT7Kj","colab_type":"text"},"source":["## Generate Text From The Pretrained Model\n","\n","If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n","\n","This is currently the only way to generate text from the 774M or 1558M models with this notebook."]},{"cell_type":"code","metadata":{"id":"hsUd_jHgUZnD","colab_type":"code","outputId":"4e0c8a3f-3527-41c4-e3fe-3357f3f8f6c2","executionInfo":{"status":"ok","timestamp":1567017356042,"user_tz":420,"elapsed":26973,"user":{"displayName":"Max Woolf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mD2mXear_qVUygpwsjIX8bFdN6WN2fKW_XEDgsFOA=s64","userId":"10954469476206133987"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["model_name = \"774M\"\n","\n","gpt2.download_gpt2(model_name=model_name)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Fetching checkpoint: 1.05Mit [00:00, 354Mit/s]                                                      \n","Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n","Fetching hparams.json: 1.05Mit [00:00, 279Mit/s]                                                    \n","Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:23, 131Mit/s]                                  \n","Fetching model.ckpt.index: 1.05Mit [00:00, 380Mit/s]                                                \n","Fetching model.ckpt.meta: 2.10Mit [00:00, 226Mit/s]                                                 \n","Fetching vocab.bpe: 1.05Mit [00:00, 199Mit/s]                                                       \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"BAe4NpKNUj2C","colab_type":"code","outputId":"b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae","executionInfo":{"status":"ok","timestamp":1567017483570,"user_tz":420,"elapsed":13453,"user":{"displayName":"Max Woolf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mD2mXear_qVUygpwsjIX8bFdN6WN2fKW_XEDgsFOA=s64","userId":"10954469476206133987"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["sess = gpt2.start_tf_sess()\n","\n","gpt2.load_gpt2(sess, model_name=model_name)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0828 18:37:58.571830 139905369159552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stderr"},{"output_type":"stream","text":["Loading pretrained model models/774M/model.ckpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-xInIZKaU104","colab_type":"code","outputId":"56348e28-7d08-45e3-c859-f26c0efd066d","executionInfo":{"status":"ok","timestamp":1567018062144,"user_tz":420,"elapsed":18497,"user":{"displayName":"Max Woolf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mD2mXear_qVUygpwsjIX8bFdN6WN2fKW_XEDgsFOA=s64","userId":"10954469476206133987"}},"colab":{"base_uri":"https://localhost:8080/","height":797}},"source":["gpt2.generate(sess,\n","              model_name=model_name,\n","              prefix=\"The secret of life is\",\n","              length=100,\n","              temperature=0.7,\n","              top_p=0.9,\n","              nsamples=5,\n","              batch_size=5\n","              )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The secret of life is that it's really easy to make it complicated,\" said Bill Nye, the host of the popular science show \"Bill Nye the Science Guy.\" \"And this is one of the reasons why we all need to be smarter about science, because we can't keep up with the amazing things that are going on all the time.\"\n","\n","While Nye is correct that \"everything that's going on all the time\" is making the world a better place, he misses the point. This is not\n","====================\n","The secret of life is in the rhythm of the universe. It's not a mystery. It's not a mystery to me. It's the nature of the universe. It's the beauty of the universe. It's the way the universe works. It's the way the universe is. It's the way the universe is going to work. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way\n","====================\n","The secret of life is in the universe.\n","\n","\n","-\n","\n","The Red Devil\n","\n","It's the end of the world as we know it, and the only thing that can save us is a band of super-powered individuals known as the Red Devil.\n","\n","\n","The Red Devil is a group of super-powered individuals who are seeking the secret of life and the only way they know how to do it is by taking on the roles of a variety of different super-powered individuals, each of which has their own\n","====================\n","The secret of life is in the mixing of the elements, and it is the mixing of the elements that makes life possible.\"\n","\n","But in the world of food science, the idea of a \"complex\" or \"complexity\" is almost entirely imaginary.\n","\n","As a scientist, I'm fascinated by the question of how life first began.\n","\n","It's the question that drives my work and the work of the scientists who work on it.\n","\n","My current research is exploring how microbes work in the first moments\n","====================\n","The secret of life is the journey of life, the search for the truth.\n","\n","4.4.2. The last thing you know\n","\n","There is nothing more important than the last thing you know.\n","\n","4.4.3. The little things that make all the difference\n","\n","The little things that make all the difference.\n","\n","4.4.4. The truth is the best teacher\n","\n","The truth is the best teacher.\n","\n","4.4.5. The truth is what\n","====================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ig-KVgkCDCKD","colab_type":"text"},"source":["# Etcetera\n","\n","If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"]},{"cell_type":"code","metadata":{"id":"rIHiVP53FnsX","colab_type":"code","colab":{}},"source":["!kill -9 -1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wmTXWNUygS5E","colab_type":"text"},"source":["# LICENSE\n","\n","MIT License\n","\n","Copyright (c) 2019 Max Woolf\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE."]}]}