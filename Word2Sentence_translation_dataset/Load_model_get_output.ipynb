{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate a given input sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required files:\n",
    "\n",
    "* ModelClasses.py -> Seq2SeqModel\n",
    "* cc_model_5p.pt -> the model state to load. Trained on ~200,000 closed captions\n",
    "* SRC_vocab.pkl -> vocabulary needed for translation\n",
    "* TRG_vocab.pkl -> vocabulary needed for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "from ModelClasses import Encoder, Decoder, Seq2Seq\n",
    "\n",
    "#Now filepaths, load vocab\n",
    "#Model path\n",
    "model_path = './cc_model_5p.pt'\n",
    "\n",
    "#Read in the source and target vocabs. Needed for translation\n",
    "read_src_vocab = pd.read_pickle(r'./SRC_vocab.pkl')\n",
    "read_trg_vocab = pd.read_pickle(r'./TRG_vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for loading model and performing translation \n",
    "\n",
    "First initializes and loads model, second translates a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_and_load_model(src_vocab, trg_vocab, model_path):\n",
    "    INPUT_DIM = len(src_vocab)\n",
    "    OUTPUT_DIM = len(trg_vocab)\n",
    "    ENC_EMB_DIM = 256\n",
    "    DEC_EMB_DIM = 256\n",
    "    HID_DIM = 512\n",
    "    ENC_DROPOUT = 0.5\n",
    "    DEC_DROPOUT = 0.5\n",
    "    \n",
    "    SEED = 1234\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = Seq2Seq(enc, dec, device).to(device)\n",
    "    model.load_state_dict(torch.load(model_path)) #load model\n",
    "    return model\n",
    "\n",
    "def translate_sentence(model, sentence):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    model.eval()\n",
    "    \n",
    "    tokenized = nlp(sentence) \n",
    "    tokenized = ['<sos>'] + [t.lower_ for t in tokenized] + ['<eos>']\n",
    "    numericalized = [read_src_vocab.stoi[t] for t in tokenized] \n",
    "    unnumericalized = [read_src_vocab.itos[t] for t in numericalized] #readable words\n",
    "\n",
    "    \n",
    "    tokenized_trg = nlp('a '*2*len(tokenized)) #just to give a target length longer than input\n",
    "    tokenized_trg = ['<sos>'] + [t.lower_ for t in tokenized_trg] + ['<eos>']\n",
    "    numericalized_trg = [read_trg_vocab.stoi[t] for t in tokenized_trg] \n",
    "    \n",
    "    sentence_length = torch.LongTensor([len(numericalized)]).to(model.device) \n",
    "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(model.device) \n",
    "\n",
    "    tensor_targ = torch.LongTensor(numericalized_trg).unsqueeze(1).to(model.device) \n",
    "    \n",
    "    translation_tensor_logits = model(tensor, tensor_targ, 0) \n",
    "    \n",
    "    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n",
    "    translation = [read_trg_vocab.itos[t] for t in translation_tensor]\n",
    " \n",
    "    # Start at the first index.  We don't need to return the <sos> token or the <eos> token\n",
    "    translation = translation[1:]\n",
    "    if '<eos>' in translation:\n",
    "        end_ind = translation.index('<eos>')\n",
    "        translation = translation[:end_ind]\n",
    "    \n",
    "    translation = \" \".join(translation) #join the list of words together as a string\n",
    "    \n",
    "    return translation, translation_tensor_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use the methods to load the stored model and vocabularies and then translate a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank you for the time .\n"
     ]
    }
   ],
   "source": [
    "#Load and intialize the model\n",
    "model = init_and_load_model(read_src_vocab, read_trg_vocab, model_path)\n",
    "\n",
    "#specify an input sentence and translate it\n",
    "sentence = 'thank for time'\n",
    "response, logits = translate_sentence(model, sentence)\n",
    "\n",
    "#Now check out the output\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
